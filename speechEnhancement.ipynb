{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjTWu0gLwqen",
        "outputId": "61a338b5-4be7-43e5-89ed-4a9e2254a39a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pypesq\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow_io as tfio\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "import tensorflow_io as tfio\n",
        "import warnings\n",
        "import glob\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from pypesq import pesq\n",
        "import soundfile as sf"
      ],
      "metadata": {
        "id": "qTdnmevYwrv7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sr=8000\n",
        "speech_length_pix_sec=27e-3\n",
        "total_length = 3.6\n",
        "trim_length = 28305\n",
        "n_fft=255\n",
        "frame_length=255\n",
        "frame_step = 110\n",
        "\n",
        "noisefiles = glob.glob('/kaggle/input/urban-sound-8k/**/*.wav')\n",
        "files= glob.glob('/kaggle/input/ravdess-8k/**/*.wav')\n",
        "print(len(files),'clean data files')\n",
        "print('Should be similar value to trim length', total_length*sr,trim_length)\n",
        "print('Should be similar value to n_fft',n_fft, int(speech_length_pix_sec*sr))\n",
        "\n",
        "@tf.function\n",
        "def load_wav(filename):\n",
        "    file_contents = tf.io.read_file(filename)\n",
        "    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)\n",
        "    wav = tf.squeeze(wav, axis=-1)\n",
        "#     sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
        "#     wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=sr)\n",
        "    return wav\n",
        "\n",
        "@tf.function\n",
        "def preprocess_tf(filepath):\n",
        "    wav = load_wav(filepath)\n",
        "    wav = wav[:trim_length]\n",
        "    zero_padding = tf.zeros([trim_length] - tf.shape(wav), dtype=tf.float32)\n",
        "    wav = tf.concat([zero_padding, wav],0)\n",
        "    return wav\n",
        "\n",
        "@tf.function\n",
        "def white_noise(data,factor=0.03):\n",
        "    noise_amp = factor*tf.reduce_max(data)*tf.random.normal(shape=(1,))\n",
        "    corr_data = data + noise_amp*tf.random.normal(shape=tf.shape(data))\n",
        "    return corr_data, data\n",
        "\n",
        "@tf.function\n",
        "def urban_noise(corr_data, data, factor=0.4,sr=sr):\n",
        "    noisefile = tf.gather(noisefiles,tf.random.uniform((),0, len(noisefiles)-1,dtype=tf.int32))\n",
        "    noisefile  = load_wav(noisefile)\n",
        "    mixed = noisefile * factor * tf.reduce_max(corr_data)/tf.reduce_max(noisefile) + corr_data\n",
        "    return mixed, data\n",
        "\n",
        "@tf.function\n",
        "def convert_to_spectrogram(wav_corr, wavclean):\n",
        "    spectrogram_corr = tf.signal.stft(wav_corr, frame_length=frame_length, fft_length=n_fft,\n",
        "                                      frame_step=frame_step)\n",
        "    spectrogram = tf.signal.stft(wavclean, frame_length=frame_length, fft_length=n_fft,\n",
        "                                      frame_step=frame_step)\n",
        "    return spectrogram_corr, spectrogram\n",
        "\n",
        "@tf.function\n",
        "def spectrogram_abs(spectrogram_corr, spectrogram):\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram_corr = tf.abs(spectrogram_corr)\n",
        "    return spectrogram_corr, spectrogram\n",
        "\n",
        "@tf.function\n",
        "def augment(spectrogram_corr, spectrogram):\n",
        "    spectrogram_corr = tfio.audio.freq_mask(spectrogram_corr, 10)\n",
        "    spectrogram_corr = tfio.audio.time_mask(spectrogram_corr, 20)\n",
        "    return spectrogram_corr, spectrogram\n",
        "\n",
        "@tf.function\n",
        "def expand_dims(spectrogram_corr, spectrogram):\n",
        "    spectrogram_corr = tf.expand_dims(spectrogram_corr, axis=2)\n",
        "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
        "    return spectrogram_corr, spectrogram"
      ],
      "metadata": {
        "id": "yq4ZemqZ0S8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=16\n",
        "train_val_split_ratio = 0.2\n",
        "split_index = int(len(files)*train_val_split_ratio)\n",
        "train_files = files[split_index:]\n",
        "val_files = files[:split_index]\n",
        "\n",
        "def configure_dataset(files, train=True):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(files)\n",
        "    dataset = dataset.map(load_wav, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.map(white_noise, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.map(urban_noise, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.map(convert_to_spectrogram, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    if not train:\n",
        "        dataset = dataset.map(expand_dims, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if train:\n",
        "        dataset = dataset.map(spectrogram_abs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.map(expand_dims, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        dataset = dataset.batch(batch_size)\n",
        "        dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "train_dataset = configure_dataset(train_files)\n",
        "val_dataset = configure_dataset(val_files)"
      ],
      "metadata": {
        "id": "ta_Mw9EY0Woh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    BatchNormalization,\n",
        "    Conv2D,\n",
        "    Conv2DTranspose,\n",
        "    MaxPooling2D,\n",
        "    Dropout,\n",
        "    SpatialDropout2D,\n",
        "    UpSampling2D,\n",
        "    Input,\n",
        "    concatenate,\n",
        "    multiply,\n",
        "    add,\n",
        "    Activation,\n",
        ")\n",
        "\n",
        "\n",
        "def upsample_conv(filters, kernel_size, strides, padding):\n",
        "    return Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)\n",
        "\n",
        "\n",
        "def upsample_simple(filters, kernel_size, strides, padding):\n",
        "    return UpSampling2D(strides)\n",
        "\n",
        "\n",
        "def attention_gate(inp_1, inp_2, n_intermediate_filters):\n",
        "    \"\"\"Attention gate. Compresses both inputs to n_intermediate_filters filters before processing.\n",
        "       Implemented as proposed by Oktay et al. in their Attention U-net, see: https://arxiv.org/abs/1804.03999.\n",
        "    \"\"\"\n",
        "    inp_1_conv = Conv2D(\n",
        "        n_intermediate_filters,\n",
        "        kernel_size=1,\n",
        "        strides=1,\n",
        "        padding=\"same\",\n",
        "        kernel_initializer=\"he_normal\",\n",
        "    )(inp_1)\n",
        "    inp_2_conv = Conv2D(\n",
        "        n_intermediate_filters,\n",
        "        kernel_size=1,\n",
        "        strides=1,\n",
        "        padding=\"same\",\n",
        "        kernel_initializer=\"he_normal\",\n",
        "    )(inp_2)\n",
        "\n",
        "    f = Activation(\"relu\")(add([inp_1_conv, inp_2_conv]))\n",
        "    g = Conv2D(\n",
        "        filters=1,\n",
        "        kernel_size=1,\n",
        "        strides=1,\n",
        "        padding=\"same\",\n",
        "        kernel_initializer=\"he_normal\",\n",
        "    )(f)\n",
        "    h = Activation(\"sigmoid\")(g)\n",
        "    return multiply([inp_1, h])\n",
        "\n",
        "\n",
        "def attention_concat(conv_below, skip_connection):\n",
        "    \"\"\"Performs concatenation of upsampled conv_below with attention gated version of skip-connection\n",
        "    \"\"\"\n",
        "    below_filters = conv_below.get_shape().as_list()[-1]\n",
        "    attention_across = attention_gate(skip_connection, conv_below, below_filters)\n",
        "    return concatenate([conv_below, attention_across])\n",
        "\n",
        "\n",
        "def conv2d_block(\n",
        "    inputs,\n",
        "    use_batch_norm=True,\n",
        "    dropout=0.3,\n",
        "    dropout_type=\"spatial\",\n",
        "    filters=16,\n",
        "    kernel_size=(3, 3),\n",
        "    activation=\"relu\",\n",
        "    kernel_initializer=\"he_normal\",\n",
        "    padding=\"same\",\n",
        "):\n",
        "\n",
        "    if dropout_type == \"spatial\":\n",
        "        DO = SpatialDropout2D\n",
        "    elif dropout_type == \"standard\":\n",
        "        DO = Dropout\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"dropout_type must be one of ['spatial', 'standard'], got {dropout_type}\"\n",
        "        )\n",
        "\n",
        "    c = Conv2D(\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        activation=activation,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        padding=padding,\n",
        "        use_bias=not use_batch_norm,\n",
        "    )(inputs)\n",
        "    if use_batch_norm:\n",
        "        c = BatchNormalization()(c)\n",
        "    if dropout > 0.0:\n",
        "        c = DO(dropout)(c)\n",
        "    c = Conv2D(\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        activation=activation,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        padding=padding,\n",
        "        use_bias=not use_batch_norm,\n",
        "    )(c)\n",
        "    if use_batch_norm:\n",
        "        c = BatchNormalization()(c)\n",
        "    return c\n",
        "\n",
        "\n",
        "def custom_unet(\n",
        "    input_shape,\n",
        "    num_classes=1,\n",
        "    activation=\"relu\",\n",
        "    use_batch_norm=True,\n",
        "    upsample_mode=\"deconv\",  # 'deconv' or 'simple'\n",
        "    dropout=0.3,\n",
        "    dropout_change_per_layer=0.0,\n",
        "    dropout_type=\"spatial\",\n",
        "    use_dropout_on_upsampling=False,\n",
        "    use_attention=False,\n",
        "    filters=16,\n",
        "    num_layers=4,\n",
        "    output_activation=\"sigmoid\",\n",
        "):  # 'sigmoid' or 'softmax'\n",
        "\n",
        "    if upsample_mode == \"deconv\":\n",
        "        upsample = upsample_conv\n",
        "    else:\n",
        "        upsample = upsample_simple\n",
        "\n",
        "    # Build U-Net model\n",
        "    inputs = Input(input_shape)\n",
        "    inputs_copy = tf.identity(inputs)\n",
        "    x = inputs / tf.reduce_max(inputs)\n",
        "\n",
        "    down_layers = []\n",
        "    for l in range(num_layers):\n",
        "        x = conv2d_block(\n",
        "            inputs=x,\n",
        "            filters=filters,\n",
        "            use_batch_norm=use_batch_norm,\n",
        "            dropout=dropout,\n",
        "            dropout_type=dropout_type,\n",
        "            activation=activation,\n",
        "        )\n",
        "        down_layers.append(x)\n",
        "        x = MaxPooling2D((2, 2))(x)\n",
        "        dropout += dropout_change_per_layer\n",
        "        filters = filters * 2  # double the number of filters with each layer\n",
        "\n",
        "    x = conv2d_block(\n",
        "        inputs=x,\n",
        "        filters=filters,\n",
        "        use_batch_norm=use_batch_norm,\n",
        "        dropout=dropout,\n",
        "        dropout_type=dropout_type,\n",
        "        activation=activation,\n",
        "    )\n",
        "\n",
        "    if not use_dropout_on_upsampling:\n",
        "        dropout = 0.0\n",
        "        dropout_change_per_layer = 0.0\n",
        "\n",
        "    for conv in reversed(down_layers):\n",
        "        filters //= 2  # decreasing number of filters with each layer\n",
        "        dropout -= dropout_change_per_layer\n",
        "        x = upsample(filters, (2, 2), strides=(2, 2), padding=\"same\")(x)\n",
        "        if use_attention:\n",
        "            x = attention_concat(conv_below=x, skip_connection=conv)\n",
        "        else:\n",
        "            x = concatenate([x, conv])\n",
        "        x = conv2d_block(\n",
        "            inputs=x,\n",
        "            filters=filters,\n",
        "            use_batch_norm=use_batch_norm,\n",
        "            dropout=dropout,\n",
        "            dropout_type=dropout_type,\n",
        "            activation=activation,\n",
        "        )\n",
        "\n",
        "    output_mask = Conv2D(num_classes, (1, 1), activation=output_activation)(x)\n",
        "    outputs = keras.layers.Multiply()([output_mask, inputs_copy])\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model"
      ],
      "metadata": {
        "id": "UOAijjlG0ZzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os,sys,inspect\n",
        "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
        "sys.path.append('..')\n",
        "from data_tools import audio_files_to_numpy, numpy_audio_to_matrix_spectrogram\n",
        "from data_display import make_3plots_spec_voice_noise, make_3plots_timeseries_voice_noise\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Sample rate chosen to read audio\n",
        "sample_rate = 8000\n",
        "\n",
        "# Minimum duration of audio files to consider\n",
        "min_duration = 1.0\n",
        "\n",
        "# Our training data will be frame of slightly above 1 second\n",
        "frame_length = 8064\n",
        "\n",
        "# hop length for clean voice files separation (no overlap)\n",
        "hop_length_frame = 8064\n",
        "\n",
        "# Choosing n_fft and hop_length_fft to have squared spectrograms\n",
        "n_fft = 255\n",
        "hop_length_fft = 63\n",
        "\n",
        "dim_square_spec = int(n_fft / 2) + 1\n",
        "\n",
        "\n",
        "validation_folder_ex = './validation'"
      ],
      "metadata": {
        "id": "c0riOpXb0dtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = custom_unet(\n",
        "    input_shape=(256, 128, 1),\n",
        "    use_batch_norm=True,\n",
        "    num_classes=1,\n",
        "    filters=16,\n",
        "    num_layers=4,\n",
        "    dropout=0.2,\n",
        "    output_activation='sigmoid')\n",
        "\n",
        "model_filename = 'model_weights.h5'\n",
        "callback_checkpoint = ModelCheckpoint(\n",
        "    model_filename,\n",
        "    verbose=1,\n",
        "    monitor='val_loss',\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True)\n",
        "\n",
        "callback_early_stop =tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0,\n",
        "    patience=4,\n",
        "    mode=\"auto\",\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "def signal_enhancement_loss(y_true, y_pred):\n",
        "    mae = tf.abs(y_true - y_pred)\n",
        "    speech_loss =  2 * tf.abs(y_true**2 - y_pred*y_true)\n",
        "    return tf.reduce_mean(mae, axis=-1) + tf.reduce_mean(speech_loss, axis=-1) # Note the `axis=-1`\n",
        "\n",
        "model.compile(optimizer='adam', loss=signal_enhancement_loss)\n",
        "model.load_weights('/kaggle/input/speech-mask-model/model_weights_custom_loss2.h5')"
      ],
      "metadata": {
        "id": "_HTqr3r8xUYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trim_length = 28305\n",
        "files_to_test = val_files\n",
        "test_dataset = configure_dataset(files_to_test,train=False)\n",
        "num = test_dataset.as_numpy_iterator()\n",
        "mae = tf.keras.losses.MeanAbsoluteError()\n",
        "pesq_with_noise = np.zeros(len(files_to_test))\n",
        "pesq_denoised = np.zeros(len(files_to_test))\n",
        "\n",
        "wav_clean_array =  np.zeros((len(files_to_test),trim_length))\n",
        "wav_corrupt_array =  np.zeros((len(files_to_test),trim_length))\n",
        "wav_correct_array =  np.zeros((len(files_to_test),trim_length))\n",
        "spec_clean_array=  np.zeros((len(files_to_test), 256, 128))\n",
        "spec_corrupt_array=  np.zeros((len(files_to_test), 256, 128))\n",
        "spec_correct_array=  np.zeros((len(files_to_test), 256, 128))\n",
        "loss_with_noise = np.zeros(len(files_to_test))\n",
        "loss_denoised = np.zeros(len(files_to_test))\n",
        "\n",
        "\n",
        "for ind in range(len(files_to_test)):\n",
        "    corr, clean = num.next()\n",
        "    corr_wav = tf.signal.inverse_stft(corr[:,:,0], frame_length=frame_length, fft_length=n_fft, frame_step=frame_step)\n",
        "    clean_wav = tf.signal.inverse_stft(clean[:,:,0], frame_length=frame_length, fft_length=n_fft, frame_step=frame_step)\n",
        "    corr_amp = np.abs(corr)\n",
        "    corrected_amp = model.predict(np.expand_dims(corr_amp,0))\n",
        "    corrected_spec = corrected_amp * np.exp(1j*np.angle(np.expand_dims(corr,0)))\n",
        "    corrected_wav = tf.signal.inverse_stft(corrected_spec[0,:,:,0], frame_length=frame_length, fft_length=n_fft, frame_step=frame_step)\n",
        "\n",
        "    pesq_with_noise[ind] = pesq(clean_wav,corr_wav,sr)\n",
        "    pesq_denoised[ind] = pesq(clean_wav,corrected_wav,sr)\n",
        "    wav_clean_array[ind] = clean_wav\n",
        "    wav_corrupt_array[ind] = corr_wav\n",
        "    wav_correct_array[ind] = corrected_wav\n",
        "    spec_clean_array[ind] = np.abs(clean[:,:,0])\n",
        "    spec_corrupt_array[ind] = np.abs(corr[:,:,0])\n",
        "    spec_correct_array[ind] = corrected_amp[0,:,:,0]\n",
        "    loss_with_noise[ind] = tf.reduce_mean(signal_enhancement_loss(np.abs(clean), corr_amp)).numpy()\n",
        "    loss_denoised[ind] =tf.reduce_mean(signal_enhancement_loss(np.abs(clean[:,:,0]), corrected_amp[0,:,:,0])).numpy()\n",
        "\n",
        "pesq_diff = pesq_denoised - pesq_with_noise\n",
        "\n",
        "print(np.mean(pesq_with_noise), np.mean(pesq_denoised),pesq_diff.mean())\n",
        "\n",
        "f'{np.mean(pesq_with_noise):.2f}, {np.mean(pesq_denoised):.2f}'"
      ],
      "metadata": {
        "id": "u2nMtxwFzlN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.system('rm -r val_results')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LcPH4qWzwYc",
        "outputId": "928e0d89-1f71-4c04-a3b2-82c2ae790581"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_dir = 'val_results'\n",
        "os.mkdir(results_dir)"
      ],
      "metadata": {
        "id": "BxRAozJWzx4A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linkcode\n",
        "fig = plt.figure()\n",
        "plt.title('PESQ improvement')\n",
        "plt.hist(pesq_diff);\n",
        "plt.xlabel('PESQ corrected - PESQ corrupted')\n",
        "plt.ylabel('Number')\n",
        "fig.savefig(results_dir+'/pesq_hist', bbox_inches='tight')"
      ],
      "metadata": {
        "id": "E4IYVTzX0gn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linkcode\n",
        "ind=np.where(pesq_diff==pesq_diff.max())[0][0]\n",
        "sf.write(results_dir +'/'+'clean_best_pesq_improvement.wav',wav_clean_array[ind],sr)\n",
        "sf.write(results_dir +'/'+'corrupt_best_pesq_improvement.wav',wav_corrupt_array[ind],sr)\n",
        "sf.write(results_dir +'/'+'correct_best_pesq_improvement.wav',wav_correct_array[ind],sr)"
      ],
      "metadata": {
        "id": "c_Wcqfuhz4Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(wav_clean_array[ind],rate=sr)\n"
      ],
      "metadata": {
        "id": "w--KdfOFz80E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(wav_clean_array[ind],rate=sr)\n",
        "Audio(wav_clean_array[ind],rate=sr)\n"
      ],
      "metadata": {
        "id": "Y1jbzVAkz9O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ind=np.where(pesq_diff==pesq_diff.min())[0][0]\n",
        "sf.write(results_dir +'/'+'clean_worst_pesq_improvement.wav',wav_clean_array[ind],sr)\n",
        "sf.write(results_dir +'/'+'corrupt_worst_pesq_improvement.wav',wav_corrupt_array[ind],sr)\n",
        "sf.write(results_dir +'/'+'correct_worst_pesq_improvement.wav',wav_correct_array[ind],sr)"
      ],
      "metadata": {
        "id": "AYzE9zcP0B5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(wav_clean_array[ind],rate=sr)\n",
        "Audio(wav_corrupt_array[ind],rate=sr)\n",
        "linkcode\n",
        "Audio(wav_correct_array[ind],rate=sr)"
      ],
      "metadata": {
        "id": "xJA_8NES0FGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ind=np.where(pesq_diff==pesq_diff.max())[0][0]\n",
        "fig,axes = plt.subplots(ncols=3,figsize=(20,10))\n",
        "vmax=spec_clean_array[ind].max()/3\n",
        "vmin=0\n",
        "plt.subplot(1,3,1)\n",
        "plt.title('Ground Truth')\n",
        "plt.imshow(spec_clean_array[ind], origin='lower',vmax=vmax,vmin=vmin)\n",
        "plt.subplot(1,3,2)\n",
        "plt.title('Ground Truth + Noise')\n",
        "plt.imshow(spec_corrupt_array[ind], origin='lower',vmax=vmax,vmin=vmin)\n",
        "plt.subplot(1,3,3)\n",
        "plt.title('Corrected')\n",
        "plt.imshow(spec_correct_array[ind], origin='lower',vmax=vmax,vmin=vmin)\n",
        "plt.colorbar()\n",
        "fig.savefig(results_dir+'/best_spec.png', bbox_inches='tight')"
      ],
      "metadata": {
        "id": "whae-JtR0Fv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ind=np.where(pesq_diff==pesq_diff.min())[0][0]\n",
        "fig,axes = plt.subplots(ncols=3,figsize=(20,10))\n",
        "vmax=spec_clean_array[ind].max()/3\n",
        "vmin=0\n",
        "plt.subplot(1,3,1)\n",
        "plt.title('Ground Truth')\n",
        "plt.imshow(spec_clean_array[ind], origin='lower',vmax=vmax,vmin=vmin)\n",
        "plt.subplot(1,3,2)\n",
        "plt.title('Ground Truth + Noise')\n",
        "plt.imshow(spec_corrupt_array[ind], origin='lower',vmax=vmax,vmin=vmin)\n",
        "plt.subplot(1,3,3)\n",
        "plt.title('Corrected')\n",
        "plt.imshow(spec_correct_array[ind], origin='lower',vmax=vmax,vmin=vmin)\n",
        "plt.colorbar()\n",
        "fig.savefig(results_dir+'/worst_spec.png', bbox_inches='tight')"
      ],
      "metadata": {
        "id": "MIwCRmwJ0Jkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ = np.expand_dims(corr_amp,0)"
      ],
      "metadata": {
        "id": "qes-A9FQ0Lks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 50\n",
        "test = model.predict(test_)"
      ],
      "metadata": {
        "id": "zmAauaBs0NWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(f'tar -cvzf train_val_results.tar.gz {results_dir}')"
      ],
      "metadata": {
        "id": "nTFitp070PbV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}